import json
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType

json_data = {
    'TYPE' : 'D',
    'DELIMITER' : '|',
    'HASHEADER' : True,
    'COLUMNS': [ 
        { 'ABC123' : { 'COLUMN_SIZE' : None, 'COLUMN_SEQUENCE' : 1, 'ALGORITHM' : None } }, 
        { 'XYZ456' : { 'COLUMN_SIZE' : None, 'COLUMN_SEQUENCE' : 2, 'ALGORITHM' : '233-SCRAMBLE-1' } } 
    ]
}

# Extracting the delimiter
delimiter = json_data['DELIMITER']

# Extracting columns information and constructing the schema
columns_info = json_data['COLUMNS']
columns = []
for col_info in columns_info:
    for col_name, col_details in col_info.items():
        columns.append((col_name, col_details['COLUMN_SEQUENCE']))

# Sorting columns based on COLUMN_SEQUENCE
columns = sorted(columns, key=lambda x: x[1])

# Extract column names in the correct sequence
column_names = [col[0] for col in columns]

# Construct the schema
schema = StructType([StructField(col_name, StringType(), True) for col_name in column_names])

# Initialize Spark session
spark = SparkSession.builder.appName("Ingest JSON Data").getOrCreate()

# Define the path to the data file
data_file_path = "path/to/your/data.txt"

# Read the data file
df = spark.read.format("csv") \
    .option("delimiter", delimiter) \
    .option("header", json_data['HASHEADER']) \
    .schema(schema) \
    .load(data_file_path)

# Show the DataFrame
df.show()
