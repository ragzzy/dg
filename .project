from pyspark.sql import SparkSession
from pyspark.sql.functions import max

# Initialize SparkSession
spark = SparkSession.builder.appName("LatestDate").getOrCreate()

# Sample data
data = [
    ("A", "X", "2023-01-01"),
    ("A", "X", "2023-02-01"),
    ("A", "Y", "2023-03-01"),
    ("B", "X", "2023-04-01"),
    ("B", "X", "2023-05-01"),
    ("B", "Y", "2023-06-01"),
]

# Create DataFrame
columns = ["col1", "col2", "col3"]
df = spark.createDataFrame(data, columns)

# Convert col3 to date type for proper comparison
df = df.withColumn("col3", df["col3"].cast("date"))

# Get the latest date for each (col1, col2) group
result_df = df.groupBy("col1", "col2").agg(max("col3").alias("latest_date"))

# Join back to get the original rows with the latest date
final_df = df.join(result_df, (df["col1"] == result_df["col1"]) & (df["col2"] == result_df["col2"]) & (df["col3"] == result_df["latest_date"]))\
             .select(df["col1"], df["col2"], df["col3"])

# Show the result
final_df.show()

# Save the final DataFrame to a Delta table partitioned by col3
final_df.write.format("delta").mode("overwrite").partitionBy("col3").save("/path/to/delta-table")

# Read the Delta table
delta_df = spark.read.format("delta").load("/path/to/delta-table")

# Get the latest date for each (col1, col2) group
latest_df = delta_df.groupBy("col1", "col2").agg(max("col3").alias("latest_date"))

# Join back to get the original rows with the latest date
result_df = delta_df.join(latest_df, (delta_df["col1"] == latest_df["col1"]) & (delta_df["col2"] == latest_df["col2"]) & (delta_df["col3"] == latest_df["latest_date"]))\
                    .select(delta_df["col1"], delta_df["col2"], delta_df["col3"])

# Show the result
result_df.show()
