from pyspark.sql.functions import input_file_name, current_timestamp

def process_txt_files(spark):
    return (spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "text")
        .load("abfss://container@account.dfs.core.windows.net/parent_directory/*.txt")
        .select(
            "value",
            input_file_name().alias("source_file"),
            current_timestamp().alias("processed_time")
        )
    )

def save_as_parquet(streaming_df):
    return (streaming_df.writeStream
        .format("parquet")
        .option("checkpointLocation", "/path/to/checkpoint")
        .option("path", "abfss://container@account.dfs.core.windows.net/output_directory")
        .partitionBy("processed_time")  # Optional: partition by processing time
        .start()
    )

def main():
    spark = SparkSession.builder.appName("TxtToParquetAutoLoader").getOrCreate()
    
    try:
        # Start the streaming process
        txt_stream = process_txt_files(spark)
        query = save_as_parquet(txt_stream)
        
        # Wait for the streaming query to finish
        query.awaitTermination()
        
    except Exception as e:
        print(f"Error in main execution: {str(e)}")
        raise
    finally:
        spark.stop()

if __name__ == "__main__":
    main()
