# Stage 1: Auto Load pointer files
def process_pointer_files(spark):
    return (spark.readStream.format("cloudFiles")
        .option("cloudFiles.format", "json")  # Assuming pointer files are in JSON format
        .option("cloudFiles.schemaLocation", "/path/to/pointer_schema")
        .load("abfss://container@account.dfs.core.windows.net/pointer_files_path")
        .select("file_path")  # Assuming the pointer file contains a 'file_path' field
    )

# Stage 2: Process files referenced by pointers
def process_referenced_files(pointer_df):
    def process_batch(batch_df, batch_id):
        for row in batch_df.collect():
            file_path = row['file_path']
            
            # Read the referenced file
            referenced_df = spark.read.format("delta").load(file_path)
            
            # Process the referenced file (add your transformations here)
            processed_df = referenced_df.select(...).filter(...).groupBy(...).agg(...)
            
            # Write the processed data
            (processed_df.write
                .format("delta")
                .mode("append")
                .saveAsTable("processed_data")
            )

    return (pointer_df.writeStream
        .foreachBatch(process_batch)
        .option("checkpointLocation", "/path/to/processing_checkpoint")
        .start()
    )

# Main execution
if __name__ == "__main__":
    spark = SparkSession.builder.appName("TwoStageAutoLoader").getOrCreate()
    
    pointer_stream = process_pointer_files(spark)
    query = process_referenced_files(pointer_stream)
    
    query.awaitTermination()
