%pip install great_expectations
import great_expectations as ge
from great_expectations.core.batch import BatchRequest

# Initialize a DataContext
context = ge.get_context()

df1 = spark.read.format("csv").option("header", "true").load("path_to_first_dataframe.csv")
df2 = spark.read.format("csv").option("header", "true").load("path_to_second_dataframe.csv")

import great_expectations.dataset.sparkdf_dataset

ge_df1 = ge.dataset.SparkDFDataset(df1)
ge_df2 = ge.dataset.SparkDFDataset(df2)

# Expect column1 to have the same values in both DataFrames
expectation_suite_name = "column_comparison_suite"

# Create a new expectation suite
suite = context.create_expectation_suite(expectation_suite_name, overwrite_existing=True)

# Add expectations for the columns in the first DataFrame
ge_df1.expect_column_values_to_be_in_set("column1", set(df2.select("column1").rdd.flatMap(lambda x: x).collect()))
ge_df1.expect_column_values_to_be_in_set("column2", set(df2.select("column2").rdd.flatMap(lambda x: x).collect()))

# Save the expectation suite
context.save_expectation_suite(expectation_suite=suite, expectation_suite_name=expectation_suite_name)

# Run validation on the first DataFrame
batch_request = BatchRequest(
    datasource_name="spark_df",
    data_connector_name="default_inferred_data_connector_name",
    data_asset_name="ge_df1",
)

results = context.run_validation_operator(
    "action_list_operator", assets_to_validate=[ge_df1], run_id="column_comparison_run"
)

# Print validation results
validation_result = results.list_validation_results()[0]
validation_result
