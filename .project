from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, col

# Initialize Spark session
spark = SparkSession.builder \
    .appName("JsonArrayToDataFrame") \
    .getOrCreate()

# JSON data
json_data = '''
{
    "otherAttribute1": "someValue",
    "otherAttribute2": "anotherValue",
    "columns": [
        {
            "randomKey1": {
                "field1": "value1",
                "field2": "value2"
            }
        },
        {
            "randomKey2": {
                "field1": "value3",
                "field2": "value4"
            }
        }
    ]
}
'''

# Create RDD and DataFrame
rdd = spark.sparkContext.parallelize([json_data])
df = spark.read.json(rdd)

# Extract the "columns" array
columns_df = df.select("columns")

# Explode and flatten the "columns" DataFrame
exploded_df = columns_df.select(explode(col("columns")).alias("key", "value"))
flattened_df = exploded_df.select(
    col("key"),
    col("value.field1").alias("field1"),
    col("value.field2").alias("field2")
)

# Extract additional attributes
attributes_df = df.select("otherAttribute1", "otherAttribute2")

# Cross join to add the attributes to each row of the flattened DataFrame
final_df = flattened_df.crossJoin(attributes_df)

# Show the result
final_df.show()
